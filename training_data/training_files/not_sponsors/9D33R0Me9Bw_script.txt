hello it's the next part of the really useful robot project that is a ros-powered robot that's the robot operating system so don't forget to check out my dedicated ros workstation build that i did a few weeks ago and also my universal ros remote which has a touch screen so i can define any interface as well as physical controls for driving those robots around manually for doing mapping and so on and also controlling robot dogs because we've got six axes on two three axis joysticks which is great now the really useful robot is controlled by roz it already does mapping and navigation using a lidar and the roz navigation stack and i've published all the cad and code for that robot it's entirely open source and all of that's on github and you can find the link in the description to this video now i fitted a utility stick to that robot and the aim in the future is to have robot arms attached to that and other sensors so today we're going to make a motorized linear section that slides up and down that stick and that's going to have an intel realsense camera fitted which we'll look at later in this video as well as a small screen [Music] [Music] i've got some of the parts here and some are already fitted to the robot but before we look at them thanks to 3d fuel for the filament for this project and lots of other projects in my channel so check out my channel for more 3d printing projects and check out 3dfuel.com at the bottom of my utility stick i've got this worm gear motor and there's an htd belt going around a 3d printed pulley here and this motor is a gibson robotics grwm2 24 volt motor it's a worm gear driven gearbox with a 12 mil output shaft with a flat on it there so really easy to fit that pulley on and on the back is an encoder which means we can position it accurately so that's looking okay i've got a plate on the back and the front of that motor to hold it nice and securely and that belt runs all the way to the top at the top of the stick is a smooth idler which is that blue piece that's 3d printed there's a piece of 8mm studding glued into there and then in each block either side i've got an eight millimeter internal diameter bearing basically a skate bearing so that runs nice and smoothly i've kept the motor at the bottom so we keep as much mass at the bottom as it is this whole thing is going to get quite top-heavy and i don't know how that's going to work out just yet unlike some cnc machines where there's an open length piece of belt that's stretched all the way across the carriage and the motor moves around with that belt wrapped around its pulley so we're keeping the metal motor here right at the bottom and of course as we run that motor one side of the belt goes up and one goes down and that means we can attach something on this v-slot extrusion with v wheels to make a linear carriage just attached to one side of that belt and that will make our linear section and that's what i've got just here which is a box with a back and a front and some grooves and some v wheels in here which will align with those v slots and run up and down just like a linear carriage so these pieces fit either side and we'll just align those with those grooves there so the back and the front go on and that makes one piece that goes up and down and what i've done on this side is make sure there's some screw holes i don't know if you can just see them there which will allow me to couple that belt onto here so as the belt goes up and down this will go up and down as well i've screwed the front panel on it's not attached to the belt yet but we can see that that runs really well with those v wheels in their slots i've made a clamp for the belt that screws on so that's got lots of teeth on which will grip the entire length of this on that belt and a little spacer because the belt doesn't quite touch the existing piece of plastic i fitted that clamp just in there so this is attached to the belt now so i've only got one thing to do before we can put the front on and that's work out how cables are going to go to this as it moves yep we're going to use one of these cable chain things which allows us to route the cables in attach one end at the bottom and then neatly constrain those cables as the carriage goes up and down and you get these on cnc machines and 3d printers i fitted the cable chain and that's fitted to the inside of the front plate on the piece that moves and to the stick at the bottom so let's put some power on that i've just got a battery for now because there's no control system and we should see as we wind that up and this unwinds nicely there's a couple of cables on here at the moment we'll talk about later so that seems to work quite well you can see the chain has unwound up the stick and going back down we need to do some cable management at the bottom but for now the mechanism works really well and we can keep those cables nice and neat without sort of like loads of trailing cables everywhere mounted on the moving part is going to be a number of things we can have a robot arm eventually that can manipulate objects but what we're going to do in this video is add an intel realsense camera that can sense depth as well as be used for a normal webcam for the jetson inference deep learning model so we can do vision recognition and we're also going to put a little screen on i've got a five inch touchscreen here and that means that we can actually see what the view is in the camera because when i do my open dog hand gesture video it's really hard to align your hand gestures with the camera view because you can't see what it is so let's have a look at some head designs this is head candidate one and of course all of these heads have a hole in the top so that the stick can go through there as it slides down so this one had a camera in the front and the screen in the front as well it's a bit angular though i thought it looked a bit star wars but also doesn't really match the aesthetic of the round base so i moved on to do another design and this is head number two so i decided that it'd be quite good if all the things were modular and they bolted onto this carriage we've already made so that we could just attach things where we wanted and the arm has space to attach at the bottom and we've got kind of the screen there and the camera on stalks attached to these sort of hubs which don't go anywhere until we've built the arm i wasn't really sure about the look of it all together though it looks a bit broken up so i thought we'd have something a bit more coherent but i did quite like these stalks that have the kind of eye attached so i moved on to design three which is round to match the aesthetic of the base and of course this one still has a hole in its head so it can move all the way down it's got the stalks to attach the camera and i've just got the screen on the front there we've also got some ears which we'll talk about in a minute [Music] i've printed and assembled most of this the only piece missing is the camera mount but we'll have a look at that in a minute that's still printing so for now i'm going to get this mounted on the robot and we'll have a look at it in some more detail so that screws to the existing chassis through the holes where the monitor mounts eventually and also on the side here where these ears are going to mount so for now i've just got these covers but eventually we might have actual ears for binaural hearing or some other feature there's quite a lot of space in here as well for electronics so it's quite good to make that accessible and that's what those covers are for right let's stick that screen in and i've got it right here it's a 5 inch hdmi display 800 by 480 pixels xpt 2046 touch controller rev3 i've had this in my spares box for ages it was originally part of project ultron for anyone who remembers that project it's got hdmi and usb power and an spi touch screen interface but probably just not going to use the touch screen and just use it as a display and to wipe that in i've used two right angle hdmi adapters so we can route the cable up the other way and we've also got a usb micro right angle cable and the ends of those are going to route back around to the back where the cables come up through that cable chain so the cables all route through they reach through the sides here and then they all pop out the back so we've got the two cables around to the bottom and we've got our hdmi and usb power for the screen now the camera is going to be the same that's going to be fitted here and the cables will run down these sides and then they can run straight out of here to the back so we're going to fit the camera this is an intel reel sense 435 i depth camera so it's a bit like a kinect but much smaller better and also the i stands for inertial measurement unit so this one's got an imu in and we can get the data as we move it around we'll do a demo in a minute but let's get that mounted in the robot that camera is fitted into this barrel and the camera's held in with a normal six mil camera mount and i've got these blue bezels on the end which fit in here so this can rotate around eventually there'll be a servo controlling it but for now there isn't and i'm going to come back and put that in later the camera is usb-c and the cable plugs in at the end there so just strain relief the cable then run it to the other end of the barrel where it exits so there's plenty of slack there for that to rotate and the cable fits under the blue cover so these covers fit on the side and there's plenty of space in the end here to put a servo to control that camera barrel so now of course that can tip up to look up at humans to do facial recognition it can look straight ahead to do hand recognition gesture recognition and we can of course see what we're doing on the screen here and it can look down past the display to see what's on the table so i think that's going to be pretty good the whole robot can rotate of course so the head doesn't rotate independently but we do need to wire it in so we've got all of these cables hanging out the back now so let's see where they're going to go hanging out the back now we've got the hdmi for the screen and the usb to power it we've got the usb for that camera and from the bottom we brought up a usb and we also brought up a power barrel jack so the best solution seems to be taking the jetson xavier out which is the main computing module running ubuntu and running ros and basically go and extend the usb cable up so i've got that usb cable here's the other end i'm going to put a usb hub in here now so we can plug in the two usbs that go to the team c and the lidar and then we're going to just run up the power with a power extension so that seems to be the simplest solution we can plug in hdmi and all the usbs at the top so usb hub fitted there with both things plugged in i've extended that power up we've got quite a lot of wires and things in now i've got this little piece of conduit here that just keeps those wires in the dead spot for the laser where the stick is so there's also a massive space here now we could put some more mass in or more batteries if it turns out to be too top heavy but we'll see how that goes in a minute there's my zavia sat in its little pocket looking nice and happy all powered up with all of those things plugged into the back eventually we'll make a cover for this but there's more of the robot to build where the robot arm fits and everything pretty sure this is where it's going to fit but we'll see how it goes i thought i should give it a go and test if it's too top heavy to drive properly now i've got most of the load on the top for this video but it seems okay i'm using the new ros remote there which you can check out so i'm just sending command velocity messages and it's all rods controlled but even if i stop quite suddenly it seems fine it doesn't tip over forward or do anything weird and remember the solid caster is on the front the back one is sprung with a suspension arm so if hindsight maybe i should have made the base differently so the drive wheels are at the back and it has two casters at the front and it's more of a square shape but actually for my purposes of research driving around looking for objects that sort of thing i think this is going to be fine i'm driving a bit too fast even to do mapping with rods here and it drives much slower when we do autonomous driving using the navigation stack so we built the map last time in episode 2 of this series so that's all done and now we can reuse that map and set a navigation goal and you can see it's moving much slower really we could have it go faster we don't really want it to so that it can achieve its goal properly and it can go and mark and clear the map as it goes which is the pink that you can see there checking it's in the right place and driving around that seems to work pretty well it's not really um got any issues there's a little bit of wobble but it's going quite slow and obviously we need machine vision to work as well so we don't want to drive too fast anyway to go and look for objects or look for markers or people's faces etc so i'm pretty happy with that i don't think it's too top heavy and i think it's fine just to carry on with the series with the bass the way it is probably time to have a look at this camera and the screen and see what we can get out of that now i thought the usb connection on this screen that provides power might actually be a usb mouse that would have been sensible so we could move the pointer but no such luck unfortunately it looks like you have to link up the spi with some drivers so i'm not going to bother with that for now but we can turn the camera on and look at it in the screen and look at the depth camera features from the intel realsense camera so the intel realsense camera comes up with three video devices one of them doesn't seem to produce a picture one is clearly lots of infrared dots it uses for depth and the other one is just the normal webcam view which we've got here so it's quite useful being able to look in the monitor and see my hands so we can do all of the gestures we can actually see where my hand is and where my face is if we're doing facial gestures so we can actually see those in the screen on the front this display is just about okay for that so we're now just running jetson inference one of the detect net examples uses the ssd mobile net model and this comes with jetson inference that basically out of the box of a jets and nano or xavier and we can see we're doing over 100 frames a second there sometimes and basically it can detect humans and 100 household objects so i'm obviously a human no problems there let's just uh move that camera down a little bit eventually they'll be motorized so you can look around yep so that's definitely a chair and i'm still a human even though i can't see my face let's see what's on the table so yeah i'm not sure why it's having difficulty with that keyboard laptop person dining table not far off cup yep definitely a cup keyboard laptop so yeah not too bad there we'll probably train our own model for specific things we want to look at i quite like to make up a kind of reduced robot universe that's just got simple objects in for it to detect like a red block and a round blue ball and everything else is totally white to make it really easy for experimentation but obviously half the fun is interacting with humans we did facial detection before for the wolverine claws and hand gestures with open dog so i think this is going to be pretty good having that camera the height of a person is really good and be able to see the view in the screen just below that works pretty well this is a pretty decent webcam but as well as that it can measure depth it's a depth camera like a microsoft kinect so one of the things i really want to do as well as detecting the object is work out how far away it is so then we can work out the kinematics for an arm and we can go and grasp it so let's power that camera up in its other mode see what we can get out of that so this is the intel realsense camera view so on the right we've got the normal rgb view we can see the imu data as well if i move this camera up and down we should be to see all those numbers so we know where the camera actually is and we can also see the depth view so wherever i put the mouse it'll actually tell me the depth down in the bottom corner here so that's quite useful if we point down and look at the dogs you can see those are slightly closer if we look at the back wall then that's further away and the objects on the wall it can measure pretty accurately so now let's look at the proper 3d view and this is the rgb overlaid onto the depth so it looks a bit funny but if i actually drag and turn around we can see what we've got is a proper 3d view as best it can do anyway and the cameras right over here where it's measuring the distance but we can see that 3d and as i scroll up and down we should be to see um some quite convincing 3d imagery but what we really need is just the ability to measure that depth so i think that's going to work pretty well there's also a ready-made node for ros so we can view that 3d color point cloud in alviz and look all around in exactly the same way as we can in the intel realsense camera viewer so that means we can do multiple things in rows including 3d mapping and navigation with the right mapping system however i'm probably not going to do that i'm just going to carry on with the navigation with my 2d laser and actually just use the camera for depth sensing to work out where objects are to manipulate them however to start with we don't even need the ros drivers all we need is the python library for the camera and that allows us to look at a specific x y coordinate in the cameras view and work out the depth to those pixels and then we can locate where the object is in 3d space using trigonometry based on the angle of the camera either from the imu data or the servo controlling it and that means literally we can say to the robot hey robot go and get that pink ball off the table it knows where the table is because it can navigate with ros to an x y coordinate on its 2d map look for the object work out where it is in 3d space hopefully grasp it with its arm using inverse kinematics and bring the object back to us and that's why it's called the really useful robot we've got quite a lot more work to do though in terms of hardware development to build the arm and control this carriage and control this and build all that into transforms in ros and also a little bit of software development to do but it doesn't look like it's going to be that difficult and it's quite interesting what you can develop just using off-the-shelf hardware and open source stuff like ros that's already out there subscribe if you want to see what happens all of this is open source and it's published on github already so if you'd like to support me through patreon or youtube channel membership then those links are in the description to this video alright that's all for now [Music] so [Music] you 